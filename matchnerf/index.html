<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>MatchNeRF</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--FACEBOOK-->
    <meta property="og:image" content="https://i.imgur.com/eREDjzw.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="922">
    <meta property="og:image:height" content="830">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://donydchen.github.io/matchnerf/" />
    <meta property="og:title" content="MatchNeRF" />
    <meta property="og:description"
        content="Project page for MatchNeRF: Explicit Correspondence Matching for Generalizable Neural Radiance Fields." />

    <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="MatchNeRF" />
    <meta name="twitter:description"
        content="Project page for MatchNeRF: Explicit Correspondence Matching for Generalizable Neural Radiance Fields." />
    <meta name="twitter:image" content="https://i.imgur.com/eREDjzw.png" />

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/theme/3024-day.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Explicit Correspondence Matching </br>
                for Generalizable Neural Radiance Fields</br>
                <small>
                    arXiv 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://donydchen.github.io/">
                            Yuedong Chen
                        </a>
                        </br>Monash University
                    </li>
                    <li>
                        <a href="https://haofeixu.github.io/">
                            Haofei Xu
                        </a>
                        </br>ETH Zurich
                    </li>
                    <li>
                        <a href="https://wuqianyi.top">
                            Qianyi Wu
                        </a>
                        </br>Monash University
                    </li>
                    <li>
                        <a href="https://www.chuanxiaz.com">
                            Chuanxia Zheng
                        </a>
                        </br>University of Oxford
                    </li><br>
                    <li>
                        <a href="https://personal.ntu.edu.sg/astjcham/">
                            Tat-Jen Cham
                        </a>
                        </br>Nanyang Technological University
                    </li>
                    <li>
                        <a href="https://jianfei-cai.github.io">
                            Jianfei Cai
                        </a>
                        </br>Monash University
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-2 col-md-offset-5 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="http://arxiv.org/abs/2304.12294">
                            <image src="img/paper.png" height="25px">
                                <h4><strong>arXiv</strong></h4>
                        </a>
                    </li>
                    <!-- <li>
                            <a href="https://youtu.be/cYr3Dz8N_9E">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Demo Video</strong></h4>
                            </a>
                        </li> -->
                    <li>
                        <a href="https://github.com/donydchen/matchnerf">
                            <image src="img/github.png" height="25px">
                                <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://huggingface.co/donydchen/matchnerf">
                            <image src="img/huggingface.svg" height="25px">
                                <h4><strong>Model</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    <image src="https://i.imgur.com/eREDjzw.png" class="img-responsive" alt="overview"
                        id="abstract_img"></image>
                    We present a new generalizable NeRF method that is able to directly generalize to new unseen
                    scenarios and perform
                    novel
                    view synthesis with as few as two source views. The key to our approach lies in the explicitly
                    modeled
                    correspondence
                    matching information, so as to provide the geometry prior to the prediction of NeRF color and
                    density for volume
                    rendering. The <b>explicit correspondence matching</b> is quantified with the cosine similarity
                    between image
                    features sampled
                    at the 2D projections of a 3D point on different views, which is able to provide reliable cues about
                    the surface
                    geometry. Unlike previous methods where image features are extracted independently for each view, we
                    consider
                    modeling
                    the <b>cross-view interactions</b> via Transformer cross-attention, which greatly improves the
                    feature matching
                    quality. Our
                    method achieves state-of-the-art results on different evaluation settings, with the experiments
                    showing a strong
                    correlation between our learned cosine feature similarity and volume density, demonstrating the
                    effectiveness and
                    superiority of our proposed method.
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Model Architecture
                </h3>
                <p style="text-align:center;">
                    <image src="https://i.imgur.com/rQN8Eg5.png" height="50px" class="img-responsive">
                </p>
                <p class="text-justify">
                    <b>MatchNeRF overview</b>. Given \(N\) input images, we extract the Transformer features and compute
                    the cosine
                    similarity in a pair-wise manner, and finally merge all pair-wise cosine similarities with
                    element-wise average.
                    <b>I)</b> For image pair \(I_i\) and \(I_j\), we first extract downsampled convolutional features
                    with a
                    weight-sharing CNN. The convolutional features are then fed into a Transformer to model cross-view
                    interactions with
                    cross-attention. <b>II)</b> To predict the color and volume density of a point on a ray for
                    volume rendering, we project the 3D point into the 2D Transformer features \(F_i\) and \(F_j\) with
                    the camera
                    parameters and bilinearly sample the feature vectors \(f_i\) and \(f_j\) at the projected locations.
                    We then
                    compute the cosine similarity \(z = \cos(f_i, f_j)\) between sampled features to encode the
                    correspondence matching information. <b>III)</b> \(z\) is next used with the 3D position
                    \(p\) and 2D view direction \(d\) for color \(c\) and density \(\sigma\) prediction. An additional
                    ray Transformer
                    is used to model cross-point interactions along a ray.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Direct inference on in-the-wild unseen scenes
                </h3>

                <div class="col-md-7" style="text-align:center;">
                    <img src="https://i.imgur.com/iVFESPu.png" class="img-responsive">
                    <p> Input </p>
                </div>
                <div class="col-md-5" style="text-align:center;">
                    <img src="https://i.imgur.com/3boKX8u.gif" id="printer_img" class="img-responsive">
                    <p> Output </p>
                </div>

                <div class="col-md-8" style="text-align:center;">
                    <img src="https://i.imgur.com/3aoAbIa.jpg" height="50px" class="img-responsive">
                    <p> Input (courtesy of <a href="https://wuqianyi.top">Qianyi</a>, taken at <a
                            href="https://monumentaustralia.org.au/themes/people/imperial/display/90817-queen-victoria">Queen
                            Victoria Monument</a>)</p>
                </div>
                <div class="col-md-3 col-md-offset-1" style="text-align:center;">
                    <img src="https://i.imgur.com/Tq07diD.gif" height="50px" class="img-responsive">
                    <p> Output </p>
                </div>

                <div class="col-md-7" style="text-align:center;">
                    <img src="https://i.imgur.com/PMVoA76.png" class="img-responsive">
                    <p> Input </p>
                </div>
                <div class="col-md-5" style="text-align:center;">
                    <img src="https://i.imgur.com/tFP6Q3p.gif" id="house_img" class="img-responsive">
                    <p> Output </p>
                </div>

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Direct inference on benchmark test scenes
                </h3>
                <div class="col-md-4" style="text-align:center;">
                    <img src="https://i.imgur.com/r2vtiaL.gif" class="img-responsive" id="dtu_img">
                    <p>DTU: Scan38, View24</p>
                </div>
                <div class="col-md-4" style="text-align:center;">
                    <img src="https://i.imgur.com/eMZjC1K.gif" class="img-responsive">
                    <p>Blender: Materials, View36</p>
                </div>
                <div class="col-md-4" style="text-align:center;">
                    <img src="https://i.imgur.com/oLaKtMX.gif" class="img-responsive" id="rff_img">
                    <p>RFF: Leaves, View13</p>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div style="text-align:center;"><a href="https://github.com/donydchen/matchnerf#rendering-video">[All
                        Results]</a></div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Quantitative comparisons
                </h3>
                <p style="text-align:center;">
                    <image src="https://i.imgur.com/v0tT5Kg.png" height="50px" class="img-responsive">
                </p>
                <p class="text-justify">
                    <b>Comparison with SOTA methods</b>. MatchNeRF performs the best for both 3- and 2-view inputs.
                    Viewpoints
                    <em>nearest</em> to the target one are selected as input. By default we measure over only <em>the
                        foreground or central
                        regions</em> following MVSNeRF's settings, while <sup>\(\top\)</sup> indicates a more accurate
                    metric by measuring over
                    <em>the whole image</em>. Default 3-view results are borrowed from MVSNeRF's paper.
                    We measure MVSNeRF's 3-view whole image results with its pretrained weight, and retrain with its
                    released code to report
                    2-view results.
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group">
                    <textarea id="bibtex" class="form-control" readonly>
@article{chen2023matchnerf,
    title={Explicit Correspondence Matching for Generalizable Neural Radiance Fields},
    author={Chen, Yuedong and Xu, Haofei and Wu, Qianyi and Zheng, Chuanxia and Cham, Tat-Jen and Cai, Jianfei},
    journal={arXiv preprint arXiv:2304.12294},
    year={2023}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    The website template was borrowed from <a href="https://jonbarron.info/mipnerf/">Mip-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>

</html>