<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="We introduce MVSplat360, a feed-forward approach for 360¬∞ novel view synthesis (NVS) of diverse real-world scenes, using only sparse observations.">
  <meta property="og:title" content="MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views" />
  <meta property="og:description"
    content="MVSplat360 addresses the challenging sparse view NVS by effectively combining geometry-aware 3D reconstruction with temporally consistent video generation." />
  <meta property="og:url" content="https://donydchen.github.io/mvsplat360" />
  <meta property="og:image" content="static/images/teaser.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="400" />

  <meta name="twitter:title" content="MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views">
  <meta name="twitter:description" content="MVSplat360 addresses the challenging sparse view NVS by effectively combining geometry-aware 3D reconstruction with temporally consistent video generation.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="feature matching, novel view synthesis, 3DGS, latent video diffusion, SVD">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>MVSplat360</title>
  <!-- Favicon generated by https://redketchup.io/favicon-generator -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon/favicon.ico">
  <link rel="apple-touch-icon" sizes="180x180" href="static/images/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="static/images/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="static/images/favicon/favicon-16x16.png">
  <link rel="manifest" href="static/images/favicon/site.webmanifest">

  <!-- Bootstrap -->
  <link href="static/css/bootstrap-4.4.1.css" rel="stylesheet">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<!--   <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script> -->

<!--   <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet"> -->

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

</head>
<body>


<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://donydchen.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Related Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://donydchen.github.io/matchnerf/" target="_blank">
            MatchNeRF (arXiv 23)
          </a>
          <a class="navbar-item" href="https://haofeixu.github.io/murf/" target="_blank">
            MuRF (CVPR 24)
          </a>
          <a class="navbar-item" href="https://donydchen.github.io/mvsplat/" target="_blank">
            MVSplat (ECCV 24 Oral)
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body" style="padding-top: 1em; padding-bottom: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title" style="margin-bottom: 0;width: 106%;margin-left: -3%;">üéûÔ∏è <span class="text-hl">MVSplat360</span>: Feed-Forward 360 Scene Synthesis from Sparse Views</h1>

          <div class="column is-full_width">
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://donydchen.github.io/">Yuedong Chen</a><sup>1</sup>,&nbsp;</span>
            <span class="author-block">
              <a href="https://www.chuanxiaz.com/">Chuanxia Zheng</a><sup>2</sup>,&nbsp;</span>
            <span class="author-block">
              <a href="https://haofeixu.github.io/">Haofei Xu</a><sup>3,4</sup>,&nbsp;</span>
            <span class="author-block">
              <a href="https://bohanzhuang.github.io/">Bohan Zhuang</a><sup>1</sup>,</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a><sup>2</sup>,&nbsp;</span>
            <span class="author-block">
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a><sup>5</sup>,&nbsp;</span>
            <span class="author-block">
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a><sup>1,5</sup></span>
          </div>

          <div class="column is-full_width">
            <h2 class="title is-4">NeurIPS 2024</h2>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Monash University,&nbsp;</span>
            <span class="author-block"><sup>2</sup>VGG, University of Oxford,&nbsp;</span>
            <span class="author-block"><sup>3</sup>ETH Zurich,&nbsp;</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>4</sup>University of T√ºbingen, T√ºbingen AI Center,&nbsp;</span>
            <span class="author-block"><sup>5</sup>Nanyang Technological University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2411.04924" target="_blank"
                   class="external-link button is-normal is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

<!--               <span class="link-block">
                <a href="static/pdfs/chen2024mvsplat360.pdf" target="_blank"
                   class="external-link button is-normal is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->

              <!-- Slides Link. -->
              <!-- <span class="link-block">
                <a href="https://haofeixu.github.io/slides/20221228_synced_unimatch.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                   <span class="icon">
                    <img width="18" alt="Colab logo" src="./static/images/slides.png"/>
                </span>
                  <span>Slides</span>
                </a>
              </span> -->

              <!-- Bilibili Link. -->
              <!-- <span class="link-block">
                <a href="https://www.bilibili.com/video/BV1uG4y1y7ms"
                   class="external-link button is-normal is-rounded is-dark disabled">
                  <span class="icon">
                    <img width="20" alt="Bilibili logo" src="https://www.bilibili.com/favicon.ico?v=1"/>
                  </span>
                  <span>Video</span>
                  </a>
              </span> -->
              <!-- Code Link. -->

              <span class="link-block">
                <a href="https://github.com/donydchen/mvsplat360" target="_blank"
                  class="external-link button is-normal is-dark disabled">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <!-- Colab Link. -->
              <!-- <span class="link-block">
                <a href="https://colab.research.google.com/drive/1r5m-xVy3Kw60U-m5VB-aQ98oqqg_6cab?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark disabled">
                  <span class="icon">
                      <img width="20" alt="Colab logo" src="./static/images/colab.png"/>
                  </span>
                  <span>Colab</span>
                  </a>
              </span> -->
              <!-- HuggingFace Link. -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/spaces/haofeixu/unimatch"
                   class="external-link button is-normal is-rounded is-dark disabled">
                  <span class="icon">
                      <img width="20" alt="HuggingFace logo" src="./static/images/huggingface.png"/>
                  </span>
                  <span>Demo</span>
                  </a>
              </span> -->
            </div>
          </div>

        </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%"
                 >
            <source src="static/videos/teaser.mp4"
                    type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <b>TL;DR:</b> MVSplat360 is a feed-forward approach for 360¬∞ novel view synthesis of diverse real-world scenes using only sparse observations.
      </h2>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Highlights -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="color: #B03B12;">Abstract</h2>
        <div class="content has-text-justified">
       We introduce MVSplat360, a feed-forward approach for 360¬∞ novel view synthesis (NVS) of diverse real-world scenes, using only sparse observations. This setting is inherently ill-posed due to minimal overlap among input views and insufficient visual information provided, making it challenging for conventional methods to achieve high-quality results. Our MVSplat360 addresses this by effectively combining geometry-aware 3D reconstruction with temporally consistent video generation. Specifically, it refactors a feed-forward 3D Gaussian Splatting (3DGS) model to render features directly into the latent space of a pre-trained Stable Video Diffusion (SVD) model, where these features then act as pose and visual cues to guide the denoising process and produce photorealistic 3D-consistent views. Our model is end-to-end trainable and supports rendering arbitrary views with as few as 5 sparse input views. To evaluate MVSplat360‚Äôs performance, we introduce a new benchmark using the challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual quality compared to state-of-the-art methods on wide-sweeping or even 360¬∞ NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the effectiveness of our model.
        </div>
      </div>
    </div>
    
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 text-hl">Overview</h2>

        <img src="static/images/architecture.png" class="center">
        <figcaption class="has-text-justified"><b>Overview of our MVSplat360.</b> (a) Given sparse posed images as input, we first match and fuse the multi-view information using a multi-view Transformer and cost volume-based encoder. (b) Next, a 3DGS representation is constructed to represent the coarse geometry of the entire scene. (c) Considering such coarse reconstruction is imperfect, we further adapt a pre-trained SVD, using features rendered from the 3DGS representation as conditions to achieve 360¬∞ novel view synthesis.</figcaption>
      </div>
    </div>

  </div>
</section>


<section class="section" style="padding-top: 0;">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered text-hl">Comparisions on DL3DV-10K</h2>

        <div class="content has-text-justified">
          <p>We present qualitative comparisons with the following state-of-the-art models:</p>
          <ul>
            <li><a href="https://davidcharatan.com/pixelsplat/">pixelSplat [Charatan et. al CVPR 2024]</a>: The pioneering feed-forward 3D Gaussians model that uses a data-driven regression architecture to predict Gaussian centers, which produces poor visual quality in large-scale scenes.</li>
            <li><a href="https://donydchen.github.io/mvsplat/">MVSplat [Chen et. al ECCV 2024]</a>: The latest published feed-forward 3DGS model that leverages a multi-view transformer and cost volume, performing well on small-scale scenes. However, it lacks generative capability, limiting its ability to handle disoccluded or unobserved regions in sparse-view large-scale scenes..</li>
            <li><a href="https://geometric-rl.mpi-inf.mpg.de/latentsplat/">latentSplat [Wewer et. al ECCV 2024]</a>: Another latest published feed-forward novel view synthesis approach that combines 3DGS with a VAE-GAN decoder. Its GAN-based framework improves visual quality, though only to a limited extent.</li>
          </ul>
        </div>

        <img src="static/images/sota_comparisons.png" class="center" alt="comparison on DL3DV-10K dataset" />
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%"
                 style="margin-top: 20px;" 
                 >
            <source src="static/videos/dl3dv_comparisons.mp4"
                    type="video/mp4">
          </video>
        </div> 

      </div>
    </div>

  </div>
</section>


<section class="section" style="padding-top: 0;">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered text-hl">Concurrent Related Works</h2>

        <div class="content">
          Since submitting our work, we‚Äôve seen a growing number of similar studies focused on novel view synthesis for large-scale scenes. Below, we list several concurrent works closely related to our MVSplat360. We encourage you to also explore them.
          <ul>
            <li><a href="https://cat3d.github.io/">CAT3D: Create Anything in 3D with Multi-View Diffusion Models</a></li>
            <li><a href="https://liuff19.github.io/ReconX/">ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model</a></li>
            <li><a href="https://drexubery.github.io/ViewCrafter/">ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis</a></li>
            <li><a href="https://runningneverstop.github.io/lm-gaussian.github.io/">LM-Gaussian: Boost Sparse-view 3D Gaussian Splatting with Large Model Priors</a></li>
            <li><a href="https://xiliu8006.github.io/3DGS-Enhancer-project/">3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors</a></li>
            <li><a href="https://arthurhero.github.io/projects/llrm/?trk=public_post_comment-text">Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats</a></li>
            <li><a href="https://haofeixu.github.io/depthsplat/">DepthSplat: Connecting Gaussian Splatting and Depth</a></li>
          </ul>
        </div>


      </div>
    </div>

  </div>
</section>

<!-- 

<section class="section" style="padding-top: 0;">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Comparisons of Cross-dataset Generalization</h2>

        <div class="content has-text-justified">
          <p>Our MVSplat is inherently superior in generalizing to <em>out-of-distribution</em> novel scenes, primarily due to the fact that the
      cost volume captures the <em>relative similarity</em> between features, which remains <em>invariant</em> compared to the absolute scale of
      features. Here, we present cross-dataset generalization by training models solely on RealEstate10K (indoor scenes), and directly 
      test them on DTU (object-centric scenes) and ACID (outdoor scenes).</p>
        </div>

        <img src="static/images/re10k_generalization.png" class="center" alt="trained on RealEstate10K, and tested on DTU and ACID" />
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%"
                 >
            <source src="static/videos/re10k_generalization.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>

  </div>
</section> -->



<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Acknowledgements</h2>
    This research is supported by the Monash FIT Start-up Grant. Dr. Chuanxia Zheng is supported by EPSRC SYN3D EP/Z001811/1.
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title" style="margin-top: -60px">BibTeX</h2>
    <pre><code>@article{chen2024mvsplat360,
    title     = {MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views},
    author    = {Chen, Yuedong and Zheng, Chuanxia and Xu, Haofei and Zhuang, Bohan and Vedaldi, Andrea and Cham, Tat-Jen and Cai, Jianfei},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    year      = {2024},
}</code></pre>
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="static/pdfs/chen2024mvsplat360.pdf" target="_blank" style="text-decoration : none">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/donydchen/mvsplat360" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            Template was borrowed from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>Please contact <a href="https://donydchen.github.io">Yuedong Chen</a> for feedback and questions.</p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
