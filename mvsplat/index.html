<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="We introduce MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images.">
  <meta property="og:title" content="MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images" />
  <meta property="og:description"
    content="Novel view synthesis via feed-forward 3D Gaussian inference from sparse multi-view images." />
  <meta property="og:url" content="https://donydchen.github.io/mvsplat" />
  <meta property="og:image" content="static/images/teaser.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="400" />

  <meta name="twitter:title" content="MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images">
  <meta name="twitter:description" content="Novel view synthesis via feed-forward 3D Gaussian inference from sparse multi-view images.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="feature matching, NeRF, novel view synthesis, 3D Gaussians, cost volume">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>MVSplat</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->

  <link rel="stylesheet" href="static/css/bootstrap.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bootstrap.bundle.min.js"></script>
</head>

<body>
  <div class="container">
    <!-- Title -->
    <h1 class="pt-5 title mb-4">MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</h1>

    <!-- <div class="mb-4" style="text-align: center; font-size: 1.5rem;">
      arXiv 2024
    </div> -->

    <div class="mb-2" style="font-size: larger; text-align: center;">
      <span class="author-block">
        <a href="https://donydchen.github.io/">Yuedong Chen</a><sup>1</sup></span>&emsp;&emsp;&emsp;
      <span class="author-block">
        <a href="https://haofeixu.github.io/">Haofei Xu</a><sup>2,3</sup></span>&emsp;&emsp;&emsp;
      <span class="author-block">
        <a href="https://www.chuanxiaz.com/">Chuanxia Zheng</a><sup>4</sup></span>&emsp;&emsp;&emsp;
      <span class="author-block">
        <a href="https://bohanzhuang.github.io/">Bohan Zhuang</a><sup>1</sup></span> <br>
      <span class="author-block">
        <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a><sup>2,5</sup></span>&emsp;&emsp;&emsp;
      <span class="author-block">
        <a href="http://www.cvlibs.net/">Andreas Geiger</a><sup>3</sup></span>&emsp;&emsp;&emsp;
      <span class="author-block">
        <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a><sup>6</sup></span>&emsp;&emsp;&emsp;
      <span class="author-block">
        <a href="https://jianfei-cai.github.io/">Jianfei Cai</a><sup>1</sup></span>
    </div>
    <div class="mb-4" style="text-align: center;">
      <span><sup>1</sup>Monash University</span>&emsp;
      <span><sup>2</sup>ETH Zurich</span>&emsp;
      <span><sup>3</sup>University of Tübingen, Tübingen AI Center</span> <br>
      <span><sup>4</sup>University of Oxford</span>&emsp;
      <span><sup>5</sup>Microsoft</span>&emsp;
      <span><sup>6</sup>Nanyang Technological University</span>
    </div>

    <div class="w-100 d-flex flex-row justify-content-center mt-4 gap-2">
      <!-- Paper PDF -->
      <a href="static/pdfs/chen2024mvsplat.pdf" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fas fa-file-pdf"></i>
        </span>
        <span>Paper</span>
      </a>
    
      <!-- Code -->
      <a href="https://github.com/donydchen/mvsplat" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
      </a>
      
      <!-- Pre-trained Models -->
      <!-- <a href="https://drive.google.com/drive/folders/14_E_5R6ojOWnLSrSVLVEMHnTiKsfddjU" target="_blank"
        class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fas fa-database"></i>
        </span>
        <span>Pre-trained Models</span>
      </a> -->
    </div>

    <!-- Teaser -->
    <div class="w-100 my-4">
      <video class="w-100 d-block" autoplay controls muted loop>
        <source src="static/videos/teaser.mp4" type="video/mp4">
      </video>
    </div>


    <div class="main-contain">
      <!-- TL;DR -->
      <h2>TL;DR</h2>
      <div class="alert alert-success tldr mb-4">
        MVSplat builds a cost volume representation to efficiently predict 3D Gaussians primitives from sparse
        multi-view images in a single forward pass.
      </div>

      <!-- Abstract -->
      <h2>Abstract</h2>
      <p class="mb-4" id="abstract">
        We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images. To
        accurately localize the Gaussian centers, we propose to build a cost volume representation via plane sweeping in the 3D
        space, where the cross-view feature similarities stored in the cost volume can provide valuable geometry cues to the
        estimation of depth. We learn the Gaussian primitives' opacities, covariances, and spherical harmonics coefficients
        jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the
        cost volume representation in learning feed-forward Gaussian Splatting models via extensive experimental evaluations. On
        the large-scale RealEstate10K and ACID benchmarks, our model achieves state-of-the-art performance with the fastest
        feed-forward inference speed (22 fps). <b>Compared to the latest state-of-the-art method pixelSplat, our model uses 10&times; 
        fewer parameters and infers more than 2&times; faster while providing higher appearance and geometry quality as
        well as better cross-dataset generalization.</b>
      </p>

      <h2>Architecture</h2>
      <figure class="mb-4">
        <img src="static/images/architecture.png" class="img-fluid teaser" alt="architecture" />
        <figcaption style="font-size: smaller;text-align: justify;"><b>Overview of MVSplat.</b> Given multiple posed images as input, we first extract
        multi-view image features with a multi-view Transformer, which contains self- and
        cross-attention layers to exchange information across views. Next, we construct per-view 
        cost volumes using plane sweeping. The Transformer features and cost volumes
        are concatenated together as input to a 2D U-Net (with cross-view attention) for cost
        volume refinement and predicting per-view depth maps. The per-view depth maps are
        unprojected to 3D and combined using a simple deterministic union operation as the 3D
        Gaussian centers. The opacity, covariance and color Gaussian parameters are predicted
        jointly with the depth maps. Finally, novel views are rendered from the predicted 3D
        Gaussians with the splatting operation.</figcaption>
      </figure>

      <!-- Comparisons -->
      <h2>Comparisons with the State-of-the-art</h2>
      <p>We present qualitative comparisons with the following state-of-the-art models:</p>
      <ul>
        <li><a href="https://davidcharatan.com/pixelsplat/">pixelSplat</a>: The latest feed-forward 3D Gaussians model
          that utilies data-driven regression architecture to predict Gaussian centers, leading to 
          poor geometry reconstruction and limited ability of cross-dataset generalization.</li>
        <li><a href="https://haofeixu.github.io/murf/">MuRF</a>: The latest feed-forward NeRF model that leverages 
          3D volume and (2+1)D CNN, which is expensive to train and renders comparably slowly.</li>
      </ul>
      <img src="static/images/sota_comparisons.png" class="img-fluid w-100 mt-2 mb-3" alt="comparison on Real Estate 10k and ACID dataset" />
      <div class="border w-100 mb-4">
        <video class="w-100 d-block" autoplay controls muted loop>
          <source src="static/videos/sota_comparisons.mp4" type="video/mp4">
        </video>
      </div>

      <!-- Geometry -->
      <h2>Comparisons of Geometry Reconstruction</h2>
      <p>Our MVSplat produces significantly higher-quality 3D Gaussian primitives than the latest state-of-the-art
        pixelSplat. The readers are invited to view the corresponding ".ply" files of the 3D Gaussians exported from
        both models provided at <b><a href="https://drive.google.com/drive/folders/1nBpUQnBvAIL7oLODElhIiLkW9x5gAuWs" target="_blank">HERE</a></b>.
        We recommend viewing them with online viewers, <i>e.g.</i>,
        <a href="https://projects.markkellogg.org/threejs/demo_gaussian_splats_3d.php?art=1&cu=0,0,1&cp=0,1,0&cla=1,0,0"
          target="_blank">3D Gaussian Splatting with Three.js</a>
        (camera up should be set to "0,0,1").
      </p>
      <img src="static/images/point_clouds.png" class="img-fluid w-100 mt-2 mb-4" alt="point clouds and depth maps" />

      <!-- cross-dataset -->
      <h2>Comparisons of Cross-dataset Generalization</h2> 
      <p>Our MVSplat is inherently superior in generalizing to <em>out-of-distribution</em> novel scenes, primarily due to the fact that the
      cost volume captures the <em>relative similarity</em> between features, which remains <em>invariant</em> compared to the absolute scale of
      features. Here, we present cross-dataset generalization by training models solely on RealEstate10K (indoor scenes), and directly 
      test them on DTU (object-centric scenes) and ACID (outdoor scenes).</p>
      <img src="static/images/re10k_generalization.png" class="img-fluid w-100 mt-2 mb-3"
        alt="trained on RealEstate10K, and tested on DTU and ACID" />
      <div class="border w-100 mb-4">
        <video class="w-100 d-block" autoplay controls muted loop>
          <source src="static/videos/re10k_generalization.mp4" type="video/mp4">
        </video>
      </div>

    <h2>BibTeX</h2>
    <pre class="mb-4"><code>@article{chen2024mvsplat,
    title     = {MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images},
    author    = {Chen, Yuedong and Xu, Haofei and Zheng, Chuanxia and Zhuang, Bohan and Pollefeys, Marc and Geiger, Andreas and Cham, Tat-Jen and Cai, Jianfei},
    journal   = {arXiv},
    year      = {2024},
}</code></pre>

    </div>

    <!-- Footer -->
    <footer class="border-top mt-5 py-4">
      This page's code uses elements from this <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
        target="_blank">Academic Project Page
        Template</a>.
    </footer>
  </div>
</body>

</html>